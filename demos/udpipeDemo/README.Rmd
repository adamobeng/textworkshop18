---
output:
  md_document:
    variant: markdown_github
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE,
  fig.path = "README-"
)
```

# udpipe demo - Jan Wijffels

* The **[udpipe R package](https://CRAN.R-project.org/package=udpipe)** is a wrapper around the **[UDPipe C++ library](https://github.com/ufal/udpipe)** and allows to do **Tokenisation, Lemmatisation, Parts of Speech tagging, morphological feature tagging and Dependency Parsing**
* Models are available for more than 50 languages. These are built on the data from http://universaldependencies.org
* Docs at https://github.com/bnosac/udpipe, https://bnosac.github.io/udpipe/en, 
* The R package allows to do the following
    + Annotation (tokenisation, lemma's, POS & Morphology tagging, Dependency Parsing)
    + Build a new annotation model based on CONLL-U data from R or easily download an existing model (>50 languages)
    + Keyword identification
    + Basic NLP flows & standard use cases of NLP 
* Example based on Customer Feedback on AirBnB apartments in Brussels (http://insideairbnb.com/get-the-data.html)


## Install R packages from CRAN

```{r, eval=FALSE}
install.packages(pkgs = c("udpipe", "textrank", "wordcloud"))
```


## Show data

```{r}
library(udpipe)
library(textrank)
library(wordcloud)
data(brussels_reviews, package = "udpipe")
comments <- subset(brussels_reviews, language %in% "es")
head(comments, n = 1)
```

## Annotate

This downloads the Spanish model and does tokenisation, lemmatisation, parts of speech tagging, morphological feature extraction and dependency parsing.

```{r, message = TRUE}
## Download + load the model
udmodel <- udpipe_download_model(language = "spanish")
udmodel <- udpipe_load_model(udmodel$file_model)
## Annotate - might take +/- 3 minutes
x <- udpipe_annotate(udmodel, x = comments$feedback, doc_id = comments$id, trace = 100) 
x <- as.data.frame(x)
head(x, 3)
```

## Find keywords

### Find keywords by doing Parts of Speech tagging in order to identify nouns

```{r}
stats <- subset(x, upos %in% c("NOUN")) 
stats <- txt_freq(stats$token)
head(stats)
```

### Find keywords based on Collocations and Co-occurrences

```{r}
## Collocation (words following one another)
stats <- keywords_collocation(x = x, 
                             term = "token", group = c("doc_id", "paragraph_id", "sentence_id"),
                             ngram_max = 4)
## Co-occurrences: How frequent do words occur in the same sentence, in this case only nouns or adjectives
stats <- cooccurrence(x = subset(x, upos %in% c("NOUN", "ADJ")), 
                     term = "lemma", group = c("doc_id", "paragraph_id", "sentence_id"))
## Co-occurrences: How frequent do words follow one another
stats <- cooccurrence(x = x$lemma, 
                     relevant = x$upos %in% c("NOUN", "ADJ"))
## Co-occurrences: How frequent do words follow one another even if we would skip 2 words in between
stats <- cooccurrence(x = x$lemma, 
                     relevant = x$upos %in% c("NOUN", "ADJ"), skipgram = 2)
head(stats)
```

```{r}
## Network visualisation - optional
library(igraph)
library(ggraph)
library(ggplot2)
wordnetwork <- head(stats, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "pink") +
  geom_node_text(aes(label = name), col = "darkgreen", size = 4) +
  theme_graph(base_family = "Arial Narrow") +
  theme(legend.position = "none") +
  labs(title = "Cooccurrences within 3 words distance", subtitle = "Nouns & Adjective")
```

### Find keywords based on the Textrank algorithm

```{r}
library(textrank)
stats <- textrank_keywords(x$lemma, 
                          relevant = x$upos %in% c("NOUN", "ADJ"), 
                          ngram_max = 8, sep = " ")
stats <- subset(stats$keywords, ngram > 1 & freq >= 5)
wordcloud(words = stats$keyword, freq = stats$freq)
```

### Find keywords based on RAKE (rapid automatic keyword extraction)

```{r}
stats <- keywords_rake(x = x, 
                      term = "token", group = c("doc_id", "paragraph_id", "sentence_id"),
                      relevant = x$upos %in% c("NOUN", "ADJ"),
                      ngram_max = 4)
head(subset(stats, freq > 3))
```

### Find keywords by looking for Phrases (noun phrases / verb phrases)

```{r}
x$phrase_tag <- as_phrasemachine(x$upos, type = "upos")
stats <- keywords_phrases(x = x$phrase_tag, term = x$token, 
                         pattern = "(A|N)+N(P+D*(A|N)*N)*", 
                         is_regex = TRUE, ngram_max = 4, detailed = FALSE)
head(subset(stats, ngram > 2))
```

### Find keywords based on results of dependency parsing (getting the subject of the text)

```{r}
stats <- merge(x, x, 
           by.x = c("doc_id", "paragraph_id", "sentence_id", "head_token_id"),
           by.y = c("doc_id", "paragraph_id", "sentence_id", "token_id"),
           all.x = TRUE, all.y = FALSE, 
           suffixes = c("", "_parent"), sort = FALSE)
stats <- subset(stats, dep_rel %in% "nsubj" & upos %in% c("NOUN") & upos_parent %in% c("ADJ"))
stats$term <- paste(stats$lemma_parent, stats$lemma, sep = " ")
stats <- txt_freq(stats$term)
wordcloud(words = stats$key, freq = stats$freq, min.freq = 3, max.words = 100,
          random.order = FALSE, colors = c("#1B9E77", "#D95F02", "#7570B3", "#E7298A", "#66A61E", "#E6AB02"))
```

## NLP flow for topicmodeling

Example flow builds a topic model at the sentence level on nouns only

```{r}
x$topic_level_id <- unique_identifier(x, fields = c("doc_id", "paragraph_id", "sentence_id"))
dtf <- subset(x, upos %in% c("NOUN"))
dtf <- document_term_frequencies(dtf, document = "topic_level_id", term = "lemma")
dtf <- document_term_frequencies_statistics(dtf)
dtf <- subset(dtf, bm25 > median(dtf$bm25))
dtm <- document_term_matrix(x = dtf[, c("doc_id", "term", "freq")])
dtm_clean <- dtm_remove_lowfreq(dtm, minfreq = 5)
dtm_clean <- dtm_remove_terms(dtm_clean, terms = c("appartement", "appart", "eter"))
dtm_clean <- dtm_remove_tfidf(dtm_clean, top = 50)

library(topicmodels)
m <- LDA(dtm_clean, k = 4, method = "Gibbs", 
         control = list(nstart = 5, burnin = 2000, best = TRUE, seed = 1:5))
scores <- predict(m, newdata = dtm, type = "topics", labels = c("labela", "labelb", "labelc", "xyz"))
tail(scores)
```

## Use dependency parsing result

Example: find the reason what is causing negative sentiment.

```{r}
x$lemma_bigram <- txt_nextgram(x$lemma, n = 2)
enriched <- merge(x, x, 
           by.x = c("doc_id", "paragraph_id", "sentence_id", "head_token_id"),
           by.y = c("doc_id", "paragraph_id", "sentence_id", "token_id"),
           all.x = TRUE, all.y = FALSE, 
           suffixes = c("", "_parent"), sort = FALSE)

lexicon <- c("caro", "sucio", "ruidoso", "ruido", "malo", "ni grande")
negative <- subset(enriched, lemma_parent %in% lexicon | lemma_bigram_parent %in% lexicon)
negative <- subset(negative, upos %in% c("NOUN"))
negative[, c("lemma_parent", "lemma")]
```

## Train your own udpipe annotator on CONLLU data

- You can train your own model based on a file on disk in CONLL-U format (e.g. from http://universaldependencies.org)
- UDPipe uses a GRU for the tokeniser, a basic neural network for the Lemma/POS/Morph tagging and a transition-based neural
dependency parser for the dependency parsing.
- Below is a toy example which will run in < 1 minute.
- Example R code available at https://github.com/bnosac/udpipe.models.ud where training was done on version 2.1 of the UD treebanks directly from R (Model training can take +/- 1 day per treebank. CPU only.)

```{r, message = TRUE}
file_conllu <- system.file(package = "udpipe", "dummydata", "traindata.conllu")

m <- udpipe_train(file = "toymodel.udpipe", 
  files_conllu_training = file_conllu,
  annotation_tokenizer = list(dimension = 16, epochs = 1, batch_size = 100, dropout = 0.7), 
  annotation_tagger = list(iterations = 1, models = 1, 
     provide_xpostag = 1, provide_lemma = 0, provide_feats = 0, 
     guesser_suffix_rules = 2, guesser_prefix_min_count = 2), 
  annotation_parser = list(iterations = 2, 
     embedding_upostag = 20, embedding_feats = 20, embedding_xpostag = 0, embedding_form = 50, 
     embedding_lemma = 0, embedding_deprel = 20, learning_rate = 0.01, 
     learning_rate_final = 0.001, l2 = 0.5, hidden_layer = 200, 
     batch_size = 10, transition_system = "projective", transition_oracle = "dynamic", 
     structured_interval = 10))
udmodel <- udpipe_load_model("toymodel.udpipe")
x <- udpipe_annotate(object = udmodel, x = "Ik ging deze morgen naar de bakker brood halen.")
x <- as.data.frame(x)
```


## Comments and feedback

Feel free to provide feedback at https://github.com/bnosac/udpipe
